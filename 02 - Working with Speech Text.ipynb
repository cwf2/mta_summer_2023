{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea8fe93",
   "metadata": {},
   "source": [
    "# Part I\n",
    "\n",
    "In the first part, we connect to the databases and collect and parse the speeches.\n",
    "\n",
    "\n",
    "## `import` statements\n",
    "\n",
    "This section loads ancillary code that isn't part of base Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94760892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code related to DICES\n",
    "from dicesapi import DicesAPI\n",
    "from dicesapi.text import CtsAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "\n",
    "# science and graphing tools\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90059704",
   "metadata": {},
   "source": [
    "## Create connections to external data sources\n",
    "\n",
    "This section instantiates two important \"objects\" and saves them to variables for later use. One, `api` is a connection to the DICES database. We'll use this to download speech data. The other, `cts`, is a connection to the Perseus Digital Library. It will be used to download the actual text of the speeches once we know their beginning and ending loci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to DICES\n",
    "api = DicesAPI(\n",
    "    logfile = 'dices.log',\n",
    "    progress_class = NotebookPBar,\n",
    ")\n",
    "\n",
    "# connection to Perseus\n",
    "cts = CtsAPI(\n",
    "    dices_api = api,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee37844",
   "metadata": {},
   "source": [
    "## Download some speeches\n",
    "\n",
    "Here we download all the speeches by Achilles from DICES. The resulting collection of data (we call it a SpeechGroup) is saved to a variable called `speeches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d09e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = api.getSpeeches(spkr_name=\"Achilles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e1bb1",
   "metadata": {},
   "source": [
    "### ðŸ¤” Sanity check\n",
    "\n",
    "#### How many speeches did we get?\n",
    "\n",
    "The command `len()` tells us the **length** of the collection, ie., how many speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf59b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(speeches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb6fef",
   "metadata": {},
   "source": [
    "#### List the speeches in tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d503bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(dict(\n",
    "        speech_id = speech.id,\n",
    "        language = speech.lang,\n",
    "        author = speech.author.name,\n",
    "        work = speech.work.title,\n",
    "        loci = speech.l_range,\n",
    ") for speech in speeches)\n",
    "\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2272a4e7",
   "metadata": {},
   "source": [
    "#### Summarize by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ccfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.groupby('language').agg(count=('speech_id', 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6ad33",
   "metadata": {},
   "source": [
    "#### Summarize by author and work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.groupby(['author', 'work']).agg(count=('speech_id', 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca937532",
   "metadata": {},
   "source": [
    "## Download the text of the speeches from Perseus\n",
    "\n",
    "In this section, we loop over all the speeches in the SpeechGroup. Our **loop variable**, here called `speech`, is set to each of the speeches in turn as we repeatedly execute all the indented commands.\n",
    "\n",
    "Within the loop, we attempt to download the text of the speech using `cts`, our connection to the Perseus Digital Library. Some of the speeches don't work: in some cases there are whole texts that aren't available from Perseus, in other cases, it's a matter of misalignment between the textual editions used by DICES versus Perseus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a progress bar: this can take a while\n",
    "pbar = NotebookPBar(max=len(speeches))\n",
    "\n",
    "for speech in speeches:\n",
    "    \n",
    "    # advance the progress bar\n",
    "    pbar.update()\n",
    "\n",
    "    # if this speech has already been downloaded, skip it\n",
    "    if hasattr(speech, 'passage') and (speech.passage is not None):\n",
    "        continue\n",
    "    \n",
    "    # otherwise, try to download\n",
    "    speech.passage = cts.getPassage(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710297f7",
   "metadata": {},
   "source": [
    "## Drop speeches for which text download failed\n",
    "\n",
    "Here we weed out any speeches for which the previous step didn't work. The final line in the loop above attempts to download the text from Perseus as a CTS Passage object, and saves the result as a new attribute of the speech, here called `speech.passage`. If this step fails, then `speech.passage` will be `None` instead of a new Passage object.\n",
    "\n",
    "#### Which ones failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Failed:')\n",
    "for s in speeches:\n",
    "    if s.passage is None:\n",
    "        print('  ', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798ecc8",
   "metadata": {},
   "source": [
    "#### Keep only speeches with a `passage` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078fa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_speeches = speeches.advancedFilter(lambda s: s.passage is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7390083",
   "metadata": {},
   "source": [
    "## Parse the text of the speeches with SpaCy\n",
    "\n",
    "In this section, we parse all the speeches with the Natural Language Processing toolkit SpaCy. For the Latin texts, we're using Patrick Burns' [LatinCy](https://huggingface.co/latincy), specifically the model `la_core_web_sm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a progress bar\n",
    "pbar = NotebookPBar(max=len(selected_speeches))\n",
    "\n",
    "for speech in selected_speeches:\n",
    "    \n",
    "    # update the progress bar\n",
    "    pbar.update()\n",
    "    \n",
    "    # run SpaCy\n",
    "    speech.passage.runSpacyPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc077f23",
   "metadata": {},
   "source": [
    "# Part II\n",
    "\n",
    "## Examining SpaCy output\n",
    "\n",
    "We've successfully run NLP on all the speeches that we could get. What do the results look like? Let's inspect the first speech a little more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2542f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = selected_speeches[0]\n",
    "speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120257c5",
   "metadata": {},
   "source": [
    "### The speech passage\n",
    "\n",
    "The DICES client collects information about the text of the speech in a special object saved as the `passage` attribute. The passage object has a couple of important attributes of its own, the most basic of which is just `text`, the plain text of the speech.\n",
    "\n",
    "Let's look at the text of Iliad 1.59-1.67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a057f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(speech.passage.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30238b",
   "metadata": {},
   "source": [
    "### SpaCy document\n",
    "\n",
    "After performing NLP, SpaCy collects information about the text in an object called a \"Document\", which is saved for us here as `.passage.spacy_doc`. One way we can use this document is as a container of **tokens**. A token is a unit of parsed languageâ€”most often a word.\n",
    "\n",
    "### Print the first 10 tokens\n",
    "\n",
    "In this simple `for` loop, we iterate over the first 10 tokens in Il. 59 *ff*. and just print them to the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3cd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in speech.passage.spacy_doc[:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb6283",
   "metadata": {},
   "source": [
    "### SpaCy tokens\n",
    "\n",
    "Each of these tokens carries a number of useful attributes:\n",
    "- `lemma_`: the dictionary headword\n",
    "- `pos_`: a universal part of speech tag\n",
    "- `morph`: a collection of morphological attributes\n",
    "\n",
    "Let's examine the first ten tokens more closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a59d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in speech.passage.spacy_doc[:10]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.morph, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f02f16",
   "metadata": {},
   "source": [
    "### All the tokens in a speech, tabular format\n",
    "\n",
    "Here we use Pandas to build a nice table with one row per token, putting the token attributes into individual columns. Using list comprehension we iterate over all the tokens in the speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d9416",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict(\n",
    "    form = token.text,\n",
    "    lemma = token.lemma_,\n",
    "    upos = token.pos_,\n",
    "    features = token.morph,\n",
    ") for token in s.passage.spacy_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef22543",
   "metadata": {},
   "source": [
    "### All the tokens in all the speeches, tabular format\n",
    "\n",
    "To create one table containing all tokens from **all** speeches, we need to do a double list-comprehension: we'll use one loop variable, `speech`, as a placeholder for the current speech, and another, `token` to iterate over all the tokens in `speech`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_table = pd.DataFrame(dict(\n",
    "    speech_id = speech.id,\n",
    "    author = speech.author.name,\n",
    "    work = speech.work.title,\n",
    "    spkr = [inst.name for inst in speech.spkr],\n",
    "    addr = [inst.name for inst in speech.addr],\n",
    "    token = token.text,\n",
    "    lemma = token.lemma_,\n",
    "    pos = token.pos_,\n",
    "    features = token.morph,\n",
    ") for speech in selected_speeches for token in speech.passage.spacy_doc)\n",
    "\n",
    "display(token_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d523f",
   "metadata": {},
   "source": [
    "### Morphological features\n",
    "\n",
    "SpaCy stores some complicated information in the `morph` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token)\n",
    "print(token.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89480ea",
   "metadata": {},
   "source": [
    "We can tease out the details a bit further and extract features we're interested in by treating `morph` as a dictionary. Instead of just printing out e.g. `Case=Acc|Gender=Masc|Number=Sing`, we can query by keyword to get one feature at a time.\n",
    "\n",
    "The `morph.to_dict()` method turns the bundle of features represented by `morph` into a Python dictionary. We can use the dictionary method `get()` to extract one feature by name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677256a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token.morph.to_dict().get('Case'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155cc5c",
   "metadata": {},
   "source": [
    "### Extracting all features of interest\n",
    "\n",
    "One of the nice things about `get()` is that if the named feature does not exist in the dictionary, we get the useful answer `None` instead of an error. So we can go ahead and ask for everything we're interested in, e.g. Mood, Case, Voice, with every single token. If a given feature isn't applicable, Python will return `None` and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c934dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_table = pd.DataFrame(dict(\n",
    "    speech_id = speech.id,\n",
    "    author = speech.author.name,\n",
    "    work = speech.work.title,\n",
    "    spkr = [inst.name for inst in speech.spkr],\n",
    "    addr = [inst.name for inst in speech.addr],\n",
    "    token = token.text,\n",
    "    lemma = token.lemma_,\n",
    "    pos = token.pos_,\n",
    "    mood = token.morph.to_dict().get('Mood'),\n",
    "    voice = token.morph.to_dict().get('Voice'),\n",
    "    person = token.morph.to_dict().get('Person'),\n",
    "    number = token.morph.to_dict().get('Number'),\n",
    "    gender = token.morph.to_dict().get('Gender'),    \n",
    "    case = token.morph.to_dict().get('Case'),\n",
    ") for speech in selected_speeches for token in speech.passage.spacy_doc)\n",
    "\n",
    "display(token_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aba391",
   "metadata": {},
   "source": [
    "### Export to Excel\n",
    "\n",
    "Let's pause here for a moment to export the full dataset to a CSV file, so that we can look at it in Excel if we want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_table.to_csv('achilles_tokens.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
