{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea8fe93",
   "metadata": {},
   "source": [
    "# Part I\n",
    "\n",
    "In the first part, we connect to the databases and collect and parse the speeches.\n",
    "\n",
    "\n",
    "## `import` statements\n",
    "\n",
    "This section loads ancillary code that isn't part of base Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94760892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code related to DICES\n",
    "from dicesapi import DicesAPI\n",
    "from dicesapi.text import CtsAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "\n",
    "# science and graphing tools\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90059704",
   "metadata": {},
   "source": [
    "## Create connections to external data sources\n",
    "\n",
    "This section instantiates two important \"objects\" and saves them to variables for later use. One, `api` is a connection to the DICES database. We'll use this to download speech data. The other, `cts`, is a connection to the Perseus Digital Library. It will be used to download the actual text of the speeches once we know their beginning and ending loci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to DICES\n",
    "api = DicesAPI(\n",
    "    logfile = 'dices.log',\n",
    "    progress_class = NotebookPBar,\n",
    ")\n",
    "\n",
    "# connection to Perseus\n",
    "cts = CtsAPI(\n",
    "    dices_api = api,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee37844",
   "metadata": {},
   "source": [
    "## Download all the speeches\n",
    "\n",
    "Here we download all the speeches from DICES using a single command. The resulting collection of data (we call it a SpeechGroup) is saved to a variable called `speeches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d09e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = api.getSpeeches(progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9766a4",
   "metadata": {},
   "source": [
    "## Select only the Latin speeches\n",
    "\n",
    "For now, let's look just at the Latin speeches. We can select a subset of `speeches` by using the `advancedFilter` method. This command takes as its argument a simple function definition. That function is then run on every one of the speeches in the SpeechGroup: any speeches for which the function returns `True` are selected; those for which it returns `False` are left behind.\n",
    "\n",
    "The function definition is created by the `lambda` keyword -- don't worry too much about the details, but basically the function we're creating here just returns `True` if the speech's `lang` tag is set to `'latin'` and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ecd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_speeches = speeches.advancedFilter(lambda s: s.lang == 'latin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b8601",
   "metadata": {},
   "source": [
    "### Sanity check: did the filter work?\n",
    "\n",
    "How many speeches are there in total? How many are in the Latin subset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total speeches:', len(speeches))\n",
    "print('latin speeches:', len(latin_speeches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca937532",
   "metadata": {},
   "source": [
    "## Download the text of the speeches from Perseus\n",
    "\n",
    "In this section, we loop over all the speeches in the SpeechGroup. Our **loop variable**, here called `speech`, is set to each of the Latin speeches in turn as we repeatedly execute all the indented commands.\n",
    "\n",
    "Within the loop, we attempt to download the text of the speech using `cts`, our connection to the Perseus Digital Library. Some of the speeches don't work: in some cases there are whole texts that aren't available from Perseus, in other cases, it's a matter of misalignment between the textual editions used by DICES versus Perseus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a progress bar: this can take a while\n",
    "pbar = NotebookPBar(max=len(latin_speeches))\n",
    "\n",
    "for speech in latin_speeches:\n",
    "    \n",
    "    # advance the progress bar\n",
    "    pbar.update()\n",
    "\n",
    "    # if this speech has already been downloaded, skip it\n",
    "    if hasattr(speech, 'passage') and (speech.passage is not None):\n",
    "        continue\n",
    "    \n",
    "    # otherwise, try to download\n",
    "    speech.passage = cts.getPassage(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710297f7",
   "metadata": {},
   "source": [
    "## Drop speeches for which text download failed\n",
    "\n",
    "Here we weed out any speeches for which the previous step didn't work. The final line in the loop above attempts to download the text from Perseus as a CTS Passage object, and saves the result as a new attribute of the speech, here called `speech.passage`. If this step fails, then `speech.passage` will be `None` instead of a new Passage object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_speeches = latin_speeches.advancedFilter(lambda s: s.passage is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('latin speeches:', len(latin_speeches))\n",
    "print('selected:', len(selected_speeches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7390083",
   "metadata": {},
   "source": [
    "## Parse the text of the speeches with SpaCy\n",
    "\n",
    "In this section, we parse all the speeches with the Natural Language Processing toolkit SpaCy. For the Latin texts, we're using Patrick Burns' [LatinCy](https://huggingface.co/latincy), specifically the model `la_core_web_sm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a progress bar\n",
    "pbar = NotebookPBar(max=len(selected_speeches))\n",
    "\n",
    "for speech in selected_speeches:\n",
    "    \n",
    "    # update the progress bar\n",
    "    pbar.update()\n",
    "    \n",
    "    # run SpaCy\n",
    "    speech.passage.runSpacyPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31690ec0",
   "metadata": {},
   "source": [
    "# Part II\n",
    "\n",
    "Now that we've got the speeches parsed, let's explore the data a little. We'll start with a single speech, Juno's speech to Aeolus in *Aeneid* 1. I happen to know its speech id is 1529."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = selected_speeches.filterIDs([1529])[0]\n",
    "print(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ce65f",
   "metadata": {},
   "source": [
    "### Speech text\n",
    "\n",
    "The plain text of the speech is stored as the `.passage.text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2542f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(speech.passage.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30238b",
   "metadata": {},
   "source": [
    "### SpaCy document\n",
    "\n",
    "After performing NLP, SpaCy collects information about the text in an object called a \"Document\", which is saved for us here as `.passage.spacy_doc`. One way we can use this document is as a container of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3cd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in speech.passage.spacy_doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb6283",
   "metadata": {},
   "source": [
    "### SpaCy tokens\n",
    "\n",
    "Each of these tokens carries a number of useful attributes:\n",
    "- `lemma_`: the dictionary headword\n",
    "- `pos_`: a universal part of speech tag\n",
    "- `morph`: a collection of morphological attributes\n",
    "\n",
    "Let's examine the first ten tokens more closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a59d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in speech.passage.spacy_doc[:10]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.morph, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
