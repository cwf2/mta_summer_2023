{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525efa98",
   "metadata": {},
   "source": [
    "This notebook assumes that we've already run **VF vs Seneca - Part 1**. If you haven't already created `token_table.csv`, you can use the cell below to execute the whole notebook and create the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run \"03 - VF vs Seneca - Part 1.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef4bb6",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "### `import` statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19753616",
   "metadata": {},
   "source": [
    "### Load the token table from Part 1\n",
    "\n",
    "Here we use Pandas to read in the CSV file that we exported at the end of the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71601b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_table = pd.read_csv('token_table.csv')\n",
    "display(token_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48313a25",
   "metadata": {},
   "source": [
    "It looks more or less the same, except that where the previous table showed missing data as Python's special value `None`, we now see `NaN` (\"not a number\"), a value used by Pandas for the same purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a8d20",
   "metadata": {},
   "source": [
    "## Summarizing the data\n",
    "\n",
    "Let's look at a couple of simple summaries of the token table. Can we count the number of tokens used by each text? By each author?\n",
    "\n",
    "We'll use the Pandas [`groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) method combined with [`agg()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html) — i.e. \"aggregate\".\n",
    "\n",
    "The first method, **groupby**, takes as its argument a column name or list of column names to group by. It produces a collection of named **groups**, subsets of the original table.\n",
    "\n",
    "The second method, **aggregate**, combines (aggregates) records to create new values. Its arguments are a little complicated. They appear as key-value pairs:\n",
    "- The key (on the left side of the `=`) is the name of a new column\n",
    "- The value is a **tuple**, a pair of values enclosed in parentheses\n",
    "    - the first value is the name of the column whose values we want to combine\n",
    "    - the second value is the name of an aggregation function\n",
    "    \n",
    "In the example below, I'm grouping by **author**. Then I'm aggregating to count the number of tokens in each group. My new column name is **n_tokens**, the column I'm aggregating is **token**, and the aggregation function is `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_table.groupby('author').agg(n_tokens=('token', 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f6128",
   "metadata": {},
   "source": [
    "Now let's break the data down in a little more detail. This time, we'll group by two columns, **author** and **title**. The column names appear as a list here. My aggregation step is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4146c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_table.groupby(['author', 'title']).agg(n_tokens=('token', 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893fb716",
   "metadata": {},
   "source": [
    "### From tables to graphs\n",
    "\n",
    "Python allows us to draw some relatively sophisticated plots right here in our notebook, with fine-grained control over things like data series, plot type, titles, legend, etc.\n",
    "\n",
    "But we can also get some quick-and-dirty plots quite easily just using our Pandas data frame. Here, I'm going to append a plotting method, `plot.barh()`, to the code we just used above to produce a quick comparison of the text lengths in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c026a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_table.groupby(['author', 'title']).agg(n_tokens=('token', 'count')).plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b7701",
   "metadata": {},
   "source": [
    ". . . But before we worry too much about customizing the output, let's first continue with exploring the data in tabular form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec26bac",
   "metadata": {},
   "source": [
    "## Counting feature frequencies\n",
    "\n",
    "At the heart of much stylometry is the counting occurrences of **features**, aspects of the text in which we're interested. These are often words, lemmata, part of speech tags, or morphological features. They can also be sub-word features like characters, phonemes, metrical quantities, etc., or larger features such as n-grams (groups of words).\n",
    "\n",
    "### Word distribution\n",
    "\n",
    "One easy place to start is with the most frequent words. [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law) tells us that the most common words occurr exponentially more frequently than less-common words.\n",
    "\n",
    "The top words tend to be so-called **function words**, words that are largely independent of the subject being written about and more related to the way the language is put together. These words are of particular interest to authorship attribution, which aims to recognize authorial style regardless of the subject.\n",
    "\n",
    "On the other end, there tends to be a very long tail of **hapax legomena**, words that are only used once. Drawing conclusions from the use of very rare words is tempting, but difficult to justify statistically.\n",
    "\n",
    "Somewhere in the middle are **content words** words that have some (more or less?) connection to the subject at hand. These words are of interest to information retrieval—for example, finding documents in an archive that are all on the same subject, despite being by different authors or written in different styles.\n",
    "\n",
    "### Counting words\n",
    "\n",
    "Let's start by calculating how many time each distinct word form occurs. Pandas data frames have a  `value_counts()` method, which counts how many times each value occurs in a given column(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = token_table.value_counts('token')\n",
    "display(token_count[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd533e",
   "metadata": {},
   "source": [
    "For our purposes here, I'd be happy to drop the punctuation marks. We can apply a filter to the original `token_table` to produce a new table that leaves out any rows where column **upos** has the value `PUNCT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punct = token_table.loc[token_table.upos != 'PUNCT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b5478",
   "metadata": {},
   "source": [
    "Now let's try calculating the top tokens again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = no_punct.value_counts('token')\n",
    "display(token_count[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23c919",
   "metadata": {},
   "source": [
    "### Does this look right?\n",
    "\n",
    "#### MFWs\n",
    "\n",
    "The most frequent words (MFW) are indeed function words, as expected. Words like *que*, *et*, *in*, *est*, *non*, can be expected to be among the top regardless of whether we're reading Cicero, Vergil, or Augustine. But as it turns out, the specific relative frequencies of these words can be a strong marker of authorial style...\n",
    "\n",
    "#### Zipf's Law\n",
    "\n",
    "If Zipf's Law holds true, then frequency decreases exponentially with rank. If we plot these counts on a log-log scale, with the most frequent at the left, then we should see a more or less straight line angling down from the upper-left to the lower-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot counts vs rank on a log-log plot\n",
    "ax.loglog(range(len(token_count)), token_count)\n",
    "\n",
    "# set some labels\n",
    "ax.set_ylabel('frequency')\n",
    "ax.set_xlabel('rank')\n",
    "ax.set_title('Token Frequency vs. Rank in VF and Seneca')\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f724ebc",
   "metadata": {},
   "source": [
    "### From inflected words to lemmata\n",
    "\n",
    "Most of the time, we aren't interested in counting, e.g., *est* and *sunt*, or *me* and *mihi*, independently. By consolidating all inflected forms of the same dictionary headword, we can reduce the number of features and increase their counts.\n",
    "\n",
    "We'll use exactly the same method, but working from the **lemma** column instead of **token**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ce7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_count = no_punct.value_counts('lemma')\n",
    "display(lemma_count[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e053f",
   "metadata": {},
   "source": [
    "### 🤔 Does this make sense?\n",
    "\n",
    "The top two tokens, *que* and *et*, appear more or less unchanged here. But the next two lemmata, *qui* and *sum*, represent productive paradigms with lots of different forms. Members show up in the top tokens, *est* at 678 for example, or *quae* at 269 and *qui* at 253. But when their individual forms are consolidated, these lemmata rise in the list.\n",
    "\n",
    "### Predominance of function words\n",
    "\n",
    "The top of the list is still dominated by function words. Just looking at this lexicon, you wouldn't guess that our corpus was made up of Imperial tragedy and epic, would you? The only noun I see in the top 25 is *manus*, and the only verbs are *sum*, *do*, and *video*, all pretty generic.\n",
    "\n",
    "### The long tail\n",
    "\n",
    "Even after lemmatization, we still have an enormous feature set here, and a large proportion of them still occur only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d19982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many lemmata?\n",
    "print(len(lemma_count), 'lemmata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many occur only once?\n",
    "n_hapax = (lemma_count==1).sum()\n",
    "print(n_hapax, 'hapax legomena')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670cbe0",
   "metadata": {},
   "source": [
    "**Wow:** half of the lemmata in our table only occur once time! These are unlikely to be useful in comparisons between texts.\n",
    "\n",
    "## More features\n",
    "\n",
    "### Part of speech tags\n",
    "\n",
    "Let's carry on with our exploration of feature counts and examine the **upos** column in our big token table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = no_punct.value_counts('upos')\n",
    "display(pos_count[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7f9a4",
   "metadata": {},
   "source": [
    "### Morphological features\n",
    "\n",
    "Similarly, we can count features derived from SpaCy's `morph` attribute. Things like mood, tense, and case can add back into the feature set information that was lost through lemmatization, but again in a consolidated way that keeps number of features low and counts high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mood\n",
    "display(no_punct.value_counts('mood'))\n",
    "\n",
    "# tense\n",
    "display(no_punct.value_counts('tense'))\n",
    "\n",
    "# case\n",
    "display(no_punct.value_counts('case'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660961bc",
   "metadata": {},
   "source": [
    "## Comparison between authors\n",
    "\n",
    "Let's see whether we can discover any quantifiable differences between Valerius Flaccus and Seneca on the basis of authorial style. Let's start by comparing the use of different parts of speech.\n",
    "\n",
    "We can use the Pandas `crosstab()` method to tabulate how many times each value in a particular column, say **upos** intersects with each value in another column, say **author**, to tabulate how many times each part of speech occurs in each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba3710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(no_punct.author, no_punct.upos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4f5f7",
   "metadata": {},
   "source": [
    "This tells us that Seneca uses more adjectives in his tragedies than Valerius Flaccus does in the *Argonautica*, but it doesn't give us a sense of how important these differences are, given the different lengths of the texts and the variation within each author.\n",
    "\n",
    "### Sampling\n",
    "\n",
    "Let's represent each author not as a single bag of words, but as a corpus of books, or, in more general terms, samples. We'll throw the **title** column into `crosstab()` along with author to cut the table more finely. This gives us a better sense of how much each author varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37255fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([no_punct.author, no_punct.title], no_punct.upos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35831558",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "We still need to adjust for the fact that some books are longer than others. To make these features comparable, we should represent them as **frequencies** rather than as raw **counts**. This process is often called **normalization**. Pandas `crosstab()` includes an option to normalize across rows in the output table, so that each row is a series of fractions that all add to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4767ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_freq = pd.crosstab([no_punct.author, no_punct.title], no_punct.upos, normalize='index')\n",
    "display(pos_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727f1e1",
   "metadata": {},
   "source": [
    "Now we can see that Seneca does indeed tend to use more adjectives than Valerius Flaccus, although their practice sometimes overlaps. One way we can represent this difference in distributions is with a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list for the labels and data values\n",
    "labels = []\n",
    "values = []\n",
    "\n",
    "# extract labels, data from grouped table\n",
    "for label, group in pos_freq.groupby('author'):\n",
    "    labels.append(label)\n",
    "    values.append(group['ADJ'])\n",
    "\n",
    "# create a new plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot the values\n",
    "ax.violinplot(values)\n",
    "\n",
    "# add labels\n",
    "ax.set_xticks(range(1, len(labels)+1), labels=labels)\n",
    "ax.set_ylabel('frequency')\n",
    "ax.set_title('adjective use by Seneca and VF')\n",
    "\n",
    "# show\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c72f8a",
   "metadata": {},
   "source": [
    "Here's a more generic version of the same code, with the column selection placed at the very top. Try changing this value to explore differences between the two authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature of interest\n",
    "feat = 'NOUN'\n",
    "\n",
    "# create empty list for the labels and data values\n",
    "labels = []\n",
    "values = []\n",
    "\n",
    "# extract labels, data from grouped table\n",
    "for label, group in pos_freq.groupby('author'):\n",
    "    labels.append(label)\n",
    "    values.append(group[feat])\n",
    "\n",
    "# create a new plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot the values\n",
    "ax.violinplot(values)\n",
    "\n",
    "# add labels\n",
    "ax.set_xticks(range(1, len(labels)+1), labels=labels)\n",
    "ax.set_ylabel('frequency')\n",
    "ax.set_title(f'{feat} use by Seneca and VF')\n",
    "\n",
    "# show\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
